{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/brendenlake/SCAN.git\n",
    "%pip install -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import NLLLoss\n",
    "from torch.optim import Adam\n",
    "from Dataloader import Dataloader, get_dataset_path\n",
    "from Encoder import EncoderRNN\n",
    "from Decoder import DecoderRNN\n",
    "from Evaluate import get_accuracy, get_accuracy_across_length\n",
    "from Train import train\n",
    "import os\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'main.ipynb'\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_config = {\n",
    "    'learning_rate': 0.001,\n",
    "    'teacher_forcing_ratio': 0.5,\n",
    "    'trials': int(1e5),\n",
    "    'gradient_clip_norm': 5,\n",
    "}\n",
    "paper_models = {\n",
    "    \"overall-best\": {\n",
    "        \"rnn-type\": \"LSTM\",\n",
    "        \"layers\": 2,\n",
    "        \"hidden_units\": 200,\n",
    "        \"dropout\": 0.5,\n",
    "        \"attention\": False,\n",
    "    },\n",
    "    \"simple-best\": {\n",
    "        \"rnn-type\": \"LSTM\",\n",
    "        \"layers\": 2,\n",
    "        \"hidden_units\": 200,\n",
    "        \"dropout\": 0,\n",
    "        \"attention\": False,\n",
    "    },\n",
    "    \"length-best\": {\n",
    "        \"rnn-type\": \"GRU\",\n",
    "        \"layers\": 1,\n",
    "        \"hidden_units\": 50,\n",
    "        \"dropout\": 0.5,\n",
    "        \"attention\": True,\n",
    "    },\n",
    "    \"add-prim-jump-best\": {\n",
    "        \"rnn-type\": \"LSTM\",\n",
    "        \"layers\": 1,\n",
    "        \"hidden_units\": 100,\n",
    "        \"dropout\": 0.1,\n",
    "        \"attention\": True,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = (\n",
    "    [\n",
    "        {\n",
    "            '_folder': 'SCAN/simple_split',\n",
    "            'dataset': 'simple',\n",
    "            'model': model,\n",
    "            'test_across_length': False,\n",
    "            '_eval_during_training': False\n",
    "        } for model in ['overall-best', 'simple-best']\n",
    "    ] + [\n",
    "        {\n",
    "            '_folder': 'SCAN/simple_split/size_variations',\n",
    "            'dataset': f'simple_p{i}',\n",
    "            'model': 'overall-best',\n",
    "            'test_across_length': False,\n",
    "            '_eval_during_training': False\n",
    "        } for i in [1, 2, 4, 8, 16, 32, 64]\n",
    "    ] + [\n",
    "        {\n",
    "            '_folder': 'SCAN/length_split',\n",
    "            'dataset': 'length',\n",
    "            'model': 'overall-best',\n",
    "            'test_across_length': True,\n",
    "            '_eval_during_training': False\n",
    "        },\n",
    "        {\n",
    "            '_folder': 'SCAN/length_split',\n",
    "            'dataset': 'length',\n",
    "            'model': 'length-best',\n",
    "            'test_across_length': False,\n",
    "            '_eval_during_training': False\n",
    "        }\n",
    "    ] + [\n",
    "        {\n",
    "            '_folder': 'SCAN/add_prim_split',\n",
    "            'dataset': 'addprim_jump',\n",
    "            'model': model_jump,\n",
    "            'test_across_length': False,\n",
    "            '_eval_during_training': False\n",
    "        } for model_jump in ['overall-best', 'add-prim-jump-best']\n",
    "    ] + [\n",
    "        {\n",
    "            '_folder': 'SCAN/add_prim_split',\n",
    "            'dataset': 'addprim_turn_left',\n",
    "            'model': model_left_turn,\n",
    "            'test_across_length': False,\n",
    "            '_eval_during_training': False\n",
    "        } for model_left_turn in ['overall-best', 'add-prim-left-turn-best']\n",
    "    ] + [\n",
    "        {\n",
    "            '_folder': 'SCAN/add_prim_split/with_additional_examples',\n",
    "            'dataset': f'addprim_complex_jump_num{num}_rep{rep}',\n",
    "            'model': 'add-prim-jump-best',\n",
    "            'test_across_length': False,\n",
    "            '_eval_during_training': False\n",
    "        }\n",
    "        for num in [1, 2, 4, 8, 16, 32]\n",
    "        for rep in [1, 2, 3, 4, 5]\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_num = -2\n",
    "config = {\n",
    "    **global_config,\n",
    "    **paper_models[\n",
    "        experiments[experiment_num]['model']\n",
    "    ],\n",
    "    **experiments[experiment_num]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = Dataloader()\n",
    "\n",
    "train_path = get_dataset_path(config['_folder'], config['dataset'], 'train')\n",
    "train_X, train_Y = dataloader.fit_transform(train_path)\n",
    "test_path = get_dataset_path(config['_folder'], config['dataset'], 'test')\n",
    "test_X, test_Y = dataloader.transform(test_path)\n",
    "\n",
    "config['input_size'] = dataloader.input_lang.n_words\n",
    "config['output_size'] = dataloader.output_lang.n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    project=\"Paper-Implemenation\",\n",
    "    entity=\"project-group-1\",\n",
    "    name=f\"Model: {config['model']}, Dataset: {config['dataset']}\",\n",
    "    config=config,\n",
    "    tags=[\"test\"]\n",
    ")\n",
    "dataloader.save(wandb.run.dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderRNN(\n",
    "    RNN_type=config['rnn-type'],\n",
    "    input_size=config['input_size'],\n",
    "    hidden_size=config['hidden_units'],\n",
    "    hidden_layers=config['layers'],\n",
    "    dropout=config['dropout']\n",
    ").to(device)\n",
    "decoder = DecoderRNN(\n",
    "    RNN_type=config['rnn-type'],\n",
    "    input_size=config['hidden_units'],\n",
    "    hidden_size=config['hidden_units'],\n",
    "    hidden_layers=config['layers'],\n",
    "    dropout=config['dropout'],\n",
    "    attention=config['attention'],\n",
    "    output_size=config['output_size']\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_optimizer = Adam(encoder.parameters(), lr=config['learning_rate'])\n",
    "decoder_optimizer = Adam(decoder.parameters(), lr=config['learning_rate'])\n",
    "criterion = NLLLoss()\n",
    "plot_every = 100\n",
    "evaluate_every = 4000  # Arbitrarily set to 100.000 / 4.000 = 25 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plot_loss_total = 0\n",
    "for iter in range(1, config['trials'] + 1):\n",
    "    index = np.random.randint(0, len(train_X))\n",
    "    input_tensor = train_X[index]\n",
    "    target_tensor = train_Y[index]\n",
    "    loss = train(\n",
    "        input_tensor=input_tensor,\n",
    "        target_tensor=target_tensor,\n",
    "        encoder=encoder,\n",
    "        decoder=decoder,\n",
    "        encoder_optimizer=encoder_optimizer,\n",
    "        decoder_optimizer=decoder_optimizer,\n",
    "        criterion=criterion,\n",
    "        input_max_length=dataloader.input_max_length,\n",
    "        teacher_forcing_ratio=config['teacher_forcing_ratio'],\n",
    "        gradient_clip_norm=config['gradient_clip_norm'],\n",
    "    )\n",
    "    plot_loss_total += loss\n",
    "    \n",
    "    if iter % plot_every == 0:\n",
    "        plot_loss_avg = plot_loss_total / plot_every\n",
    "        wandb.log({\"loss\": plot_loss_avg, \"progress\": iter / config['trials']})\n",
    "        plot_loss_total = 0\n",
    "\n",
    "    if config['_eval_during_training'] and iter % evaluate_every == 0:\n",
    "        wandb.log({\n",
    "            \"iter\": iter,\n",
    "            \"eval_during_training\": get_accuracy(\n",
    "                test_X=test_X,\n",
    "                test_Y=test_Y,\n",
    "                encoder=encoder,\n",
    "                decoder=decoder,\n",
    "                input_max_length=dataloader.input_max_length\n",
    "            )\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.summary['test_accuracy'] = get_accuracy(\n",
    "    test_X=test_X,\n",
    "    test_Y=test_Y,\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    "    input_max_length=dataloader.input_max_length\n",
    ")\n",
    "wandb.summary['training_accuracy'] = get_accuracy(\n",
    "    test_X=train_X,\n",
    "    test_Y=train_Y,\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    "    input_max_length=dataloader.input_max_length\n",
    ")\n",
    "\n",
    "if config['test_across_length']:\n",
    "    input_table = wandb.Table(\n",
    "        data=get_accuracy_across_length(\n",
    "            filter=test_X,  # Across test_X\n",
    "            test_X=test_X,\n",
    "            test_Y=test_Y,\n",
    "            encoder=encoder,\n",
    "            decoder=decoder,\n",
    "            input_max_length=dataloader.input_max_length\n",
    "        ),\n",
    "        columns=[\"length\", \"accuracy\"]\n",
    "    )\n",
    "    output_table = wandb.Table(\n",
    "        data=get_accuracy_across_length(\n",
    "            filter=test_Y,  # Across test_Y\n",
    "            test_X=test_X,\n",
    "            test_Y=test_Y,\n",
    "            encoder=encoder,\n",
    "            decoder=decoder,\n",
    "            input_max_length=dataloader.input_max_length\n",
    "        ),\n",
    "        columns=[\"length\",\n",
    "                 \"accuracy\"]\n",
    "    )\n",
    "    wandb.log({\n",
    "        \"input-zero-shot\": wandb.plot.bar(\n",
    "            table=input_table,\n",
    "            label=\"length\",\n",
    "            value=\"accuracy\",\n",
    "            title=\"Command length vs. accuracy\"\n",
    "        ),\n",
    "        \"output-zero-shot\": wandb.plot.bar(\n",
    "            table=output_table,\n",
    "            label=\"length\",\n",
    "            value=\"accuracy\",\n",
    "            title=\"Action sequence length vs. accuracy\"\n",
    "        ),\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encoder.state_dict(), os.path.join(wandb.run.dir, \"encoder.pt\"))\n",
    "torch.save(decoder.state_dict(), os.path.join(wandb.run.dir, \"decoder.pt\"))\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "59669bb1168edc622bb4dae0f982b2741a2a7b3cacb44a90a5e1c2622e59ac77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
